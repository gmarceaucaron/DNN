<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.9.1"/>
<title>DNN: nn_ops.hpp File Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">DNN
   &#160;<span id="projectnumber">1.0</span>
   </div>
   <div id="projectbrief">Lightweight library in C for training riemannian neural networks</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.9.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div id="navrow2" class="tabs2">
    <ul class="tablist">
      <li><a href="files.html"><span>File&#160;List</span></a></li>
      <li><a href="globals.html"><span>File&#160;Members</span></a></li>
    </ul>
  </div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle">
<div class="title">nn_ops.hpp File Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>Implementation of the functions required for neural networks.  
<a href="#details">More...</a></p>
<div class="textblock"><code>#include &lt;functional&gt;</code><br />
<code>#include &lt;Eigen/Dense&gt;</code><br />
<code>#include &lt;algorithm&gt;</code><br />
<code>#include &lt;assert.h&gt;</code><br />
<code>#include &lt;limits&gt;</code><br />
<code>#include &quot;<a class="el" href="utils_8hpp_source.html">utils.hpp</a>&quot;</code><br />
</div><div class="textblock"><div class="dynheader">
Include dependency graph for nn_ops.hpp:</div>
<div class="dyncontent">
<div class="center"><img src="nn__ops_8hpp__incl.png" border="0" usemap="#nn__ops_8hpp" alt=""/></div>
<map name="nn__ops_8hpp" id="nn__ops_8hpp">
<area shape="rect" id="node7" href="utils_8hpp.html" title="Implementation of auxiliary functions required for dataset loading and outputting. " alt="" coords="448,80,519,107"/></map>
</div>
</div><div class="textblock"><div class="dynheader">
This graph shows which files directly or indirectly include this file:</div>
<div class="dyncontent">
<div class="center"><img src="nn__ops_8hpp__dep__incl.png" border="0" usemap="#nn__ops_8hppdep" alt=""/></div>
<map name="nn__ops_8hppdep" id="nn__ops_8hppdep">
<area shape="rect" id="node2" href="nn__ops__conv_8hpp.html" title="Implementation of the functions required for convolutional neural networks. " alt="" coords="68,80,192,107"/><area shape="rect" id="node5" href="riemann_8cpp.html" title="main function for launching experiments with neural networks " alt="" coords="217,80,310,107"/><area shape="rect" id="node3" href="conv__fd_test_8cpp.html" title="main function for testing backpropagation for convolutional neural networks " alt="" coords="5,155,124,181"/><area shape="rect" id="node4" href="convnet_8cpp.html" title="main function for launching experiments with convolutional neural networks " alt="" coords="149,155,242,181"/></map>
</div>
</div>
<p><a href="nn__ops_8hpp_source.html">Go to the source code of this file.</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:a90b8525c0edded9e3420dc56dbb9332f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a90b8525c0edded9e3420dc56dbb9332f">initWeight</a> (const unsigned n_act0, const unsigned n_act1, const double sigma, MyMatrix &amp;W)</td></tr>
<tr class="memdesc:a90b8525c0edded9e3420dc56dbb9332f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initialize the weights of a layer with a reweighted normal noise.  <a href="#a90b8525c0edded9e3420dc56dbb9332f">More...</a><br /></td></tr>
<tr class="separator:a90b8525c0edded9e3420dc56dbb9332f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2a07778f5ff97c5d5581276d663ede6e"><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a2a07778f5ff97c5d5581276d663ede6e">initNetwork</a> (const std::vector&lt; unsigned &gt; &amp;nn_arch, const std::string act_func, std::vector&lt; MyMatrix &gt; &amp;W, std::vector&lt; MyVector &gt; &amp;B)</td></tr>
<tr class="memdesc:a2a07778f5ff97c5d5581276d663ede6e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Initialize the parameters of the Neural Network computational graph.  <a href="#a2a07778f5ff97c5d5581276d663ede6e">More...</a><br /></td></tr>
<tr class="separator:a2a07778f5ff97c5d5581276d663ede6e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a22f109ce3d23796de9d34fb9eee73040"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a22f109ce3d23796de9d34fb9eee73040">signFunc</a> (double x)</td></tr>
<tr class="memdesc:a22f109ce3d23796de9d34fb9eee73040"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the sign function.  <a href="#a22f109ce3d23796de9d34fb9eee73040">More...</a><br /></td></tr>
<tr class="separator:a22f109ce3d23796de9d34fb9eee73040"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a03a2f1c37574a2bc61630b6e5bf86dcf"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a03a2f1c37574a2bc61630b6e5bf86dcf">squareFunc</a> (double x)</td></tr>
<tr class="memdesc:a03a2f1c37574a2bc61630b6e5bf86dcf"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the square function.  <a href="#a03a2f1c37574a2bc61630b6e5bf86dcf">More...</a><br /></td></tr>
<tr class="separator:a03a2f1c37574a2bc61630b6e5bf86dcf"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6ceeb10733d64ab2587e136e884c2ec8"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a6ceeb10733d64ab2587e136e884c2ec8">sqrtFunc</a> (double x)</td></tr>
<tr class="memdesc:a6ceeb10733d64ab2587e136e884c2ec8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the square-root function.  <a href="#a6ceeb10733d64ab2587e136e884c2ec8">More...</a><br /></td></tr>
<tr class="separator:a6ceeb10733d64ab2587e136e884c2ec8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a6bac3899a024aa93b77e99e169f1e86c"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a6bac3899a024aa93b77e99e169f1e86c">sumCstFunc</a> (double x, double cst)</td></tr>
<tr class="memdesc:a6bac3899a024aa93b77e99e169f1e86c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the translation function.  <a href="#a6bac3899a024aa93b77e99e169f1e86c">More...</a><br /></td></tr>
<tr class="separator:a6bac3899a024aa93b77e99e169f1e86c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4c86f2557fe76d7b0ca7c5111eb7927b"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a4c86f2557fe76d7b0ca7c5111eb7927b">softmax</a> (const MyMatrix &amp;a, MyMatrix &amp;out)</td></tr>
<tr class="memdesc:a4c86f2557fe76d7b0ca7c5111eb7927b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the softmax function Take a matrix (n_example X n_class) of values and return a matrix (n_example X n_class) where each line is a probability distribution over the classes.  <a href="#a4c86f2557fe76d7b0ca7c5111eb7927b">More...</a><br /></td></tr>
<tr class="separator:a4c86f2557fe76d7b0ca7c5111eb7927b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1ee99de11ae8d8f50b32c2037ffd1d1b"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a1ee99de11ae8d8f50b32c2037ffd1d1b">logistic</a> (const bool deriv_flag, const MyMatrix &amp;z, MyMatrix &amp;a, MyMatrix &amp;da)</td></tr>
<tr class="memdesc:a1ee99de11ae8d8f50b32c2037ffd1d1b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the logistic function and its derivative.  <a href="#a1ee99de11ae8d8f50b32c2037ffd1d1b">More...</a><br /></td></tr>
<tr class="separator:a1ee99de11ae8d8f50b32c2037ffd1d1b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa8e7c94ef2b705a126e52d0ea2fa9901"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#aa8e7c94ef2b705a126e52d0ea2fa9901">my_tanh</a> (const bool deriv_flag, const MyMatrix &amp;z, MyMatrix &amp;a, MyMatrix &amp;da)</td></tr>
<tr class="memdesc:aa8e7c94ef2b705a126e52d0ea2fa9901"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the tanh function and its derivative.  <a href="#aa8e7c94ef2b705a126e52d0ea2fa9901">More...</a><br /></td></tr>
<tr class="separator:aa8e7c94ef2b705a126e52d0ea2fa9901"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af47e86a1783f4c04ee25301f3ec37866"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#af47e86a1783f4c04ee25301f3ec37866">relu</a> (const bool deriv_flag, const MyMatrix &amp;z, MyMatrix &amp;a, MyMatrix &amp;da)</td></tr>
<tr class="memdesc:af47e86a1783f4c04ee25301f3ec37866"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the rectified linear unit (ReLU) function and its derivative.  <a href="#af47e86a1783f4c04ee25301f3ec37866">More...</a><br /></td></tr>
<tr class="separator:af47e86a1783f4c04ee25301f3ec37866"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac56dfda8be864dadf5db85c56978171d"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#ac56dfda8be864dadf5db85c56978171d">dropout</a> (const double prob, MyMatrix &amp;A, MyMatrix &amp;B)</td></tr>
<tr class="memdesc:ac56dfda8be864dadf5db85c56978171d"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the dropout regularization technique.  <a href="#ac56dfda8be864dadf5db85c56978171d">More...</a><br /></td></tr>
<tr class="separator:ac56dfda8be864dadf5db85c56978171d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afe6393d9948ae9a81cb1292aad353c48"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#afe6393d9948ae9a81cb1292aad353c48">dropout</a> (const double prob, MyMatrix &amp;A)</td></tr>
<tr class="memdesc:afe6393d9948ae9a81cb1292aad353c48"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the dropout regularization technique.  <a href="#afe6393d9948ae9a81cb1292aad353c48">More...</a><br /></td></tr>
<tr class="separator:afe6393d9948ae9a81cb1292aad353c48"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a333e03b7cdb8f33e9619edf27f56d6cb"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a333e03b7cdb8f33e9619edf27f56d6cb">fprop</a> (const bool dropout_flag, const double dropout_prob, const ActivationFunction &amp;act_func, const std::vector&lt; MyMatrix &gt; &amp;W, const std::vector&lt; MyVector &gt; &amp;B, const MyMatrix &amp;X_batch, std::vector&lt; MyMatrix &gt; &amp;Z, std::vector&lt; MyMatrix &gt; &amp;A, std::vector&lt; MyMatrix &gt; &amp;dA)</td></tr>
<tr class="memdesc:a333e03b7cdb8f33e9619edf27f56d6cb"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the forward propagation algorithm.  <a href="#a333e03b7cdb8f33e9619edf27f56d6cb">More...</a><br /></td></tr>
<tr class="separator:a333e03b7cdb8f33e9619edf27f56d6cb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a1c7cf6b20b44ba081abe2a5e0110a599"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a1c7cf6b20b44ba081abe2a5e0110a599">bprop</a> (const std::vector&lt; MyMatrix &gt; &amp;W, const std::vector&lt; MyMatrix &gt; &amp;dA, std::vector&lt; MyMatrix &gt; &amp;gradB)</td></tr>
<tr class="memdesc:a1c7cf6b20b44ba081abe2a5e0110a599"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the backpropagation algorithm.  <a href="#a1c7cf6b20b44ba081abe2a5e0110a599">More...</a><br /></td></tr>
<tr class="separator:a1c7cf6b20b44ba081abe2a5e0110a599"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aaa5dfcccaa4f859b162005465984f40e"><td class="memTemplParams" colspan="2">template&lt;class T &gt; </td></tr>
<tr class="memitem:aaa5dfcccaa4f859b162005465984f40e"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#aaa5dfcccaa4f859b162005465984f40e">updateParam</a> (const double eta, const std::string regularizer, const double lambda, const T &amp;dparams, T &amp;params)</td></tr>
<tr class="memdesc:aaa5dfcccaa4f859b162005465984f40e"><td class="mdescLeft">&#160;</td><td class="mdescRight">Templated function for updating matrices or vectors.  <a href="#aaa5dfcccaa4f859b162005465984f40e">More...</a><br /></td></tr>
<tr class="separator:aaa5dfcccaa4f859b162005465984f40e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac8a4ddb1d085a6e8f288882dc5e2273f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#ac8a4ddb1d085a6e8f288882dc5e2273f">update</a> (const double eta, const std::vector&lt; MyMatrix &gt; &amp;gradB, const std::vector&lt; MyMatrix &gt; &amp;A, const MyMatrix &amp;X_batch, const std::string regularizer, const double lambda, std::vector&lt; MyMatrix &gt; &amp;W, std::vector&lt; MyVector &gt; &amp;B)</td></tr>
<tr class="memdesc:ac8a4ddb1d085a6e8f288882dc5e2273f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the update function of the gradient descent algorithm.  <a href="#ac8a4ddb1d085a6e8f288882dc5e2273f">More...</a><br /></td></tr>
<tr class="separator:ac8a4ddb1d085a6e8f288882dc5e2273f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a3c2822a2bbb5ca3701af33bbc402e54f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a3c2822a2bbb5ca3701af33bbc402e54f">adagradUpdate</a> (const double eta, const std::vector&lt; MyMatrix &gt; &amp;gradB, const std::vector&lt; MyMatrix &gt; &amp;A, const MyMatrix &amp;X_batch, const std::string regularizer, const double lambda, const double mat_reg, const double autocorr, std::vector&lt; MyMatrix &gt; &amp;W, std::vector&lt; MyVector &gt; &amp;B, std::vector&lt; MyMatrix &gt; &amp;mu_dW, std::vector&lt; MyVector &gt; &amp;mu_dB)</td></tr>
<tr class="memdesc:a3c2822a2bbb5ca3701af33bbc402e54f"><td class="mdescLeft">&#160;</td><td class="mdescRight">function that performs the adagrad update rule  <a href="#a3c2822a2bbb5ca3701af33bbc402e54f">More...</a><br /></td></tr>
<tr class="separator:a3c2822a2bbb5ca3701af33bbc402e54f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a9a994e746ada8c2bd3f11b68114db234"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a9a994e746ada8c2bd3f11b68114db234">computeLoss</a> (const ActivationFunction &amp;act_func, const MyMatrix &amp;X, const MyVector &amp;Y, const std::vector&lt; MyMatrix &gt; &amp;W, const std::vector&lt; MyVector &gt; &amp;B, double &amp;loss, double &amp;accuracy)</td></tr>
<tr class="memdesc:a9a994e746ada8c2bd3f11b68114db234"><td class="mdescLeft">&#160;</td><td class="mdescRight">function that compute the Negative Log-likelihood and the accuracy of the model  <a href="#a9a994e746ada8c2bd3f11b68114db234">More...</a><br /></td></tr>
<tr class="separator:a9a994e746ada8c2bd3f11b68114db234"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad4e3600f69eecf523fb78bb9ffd4ee24"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#ad4e3600f69eecf523fb78bb9ffd4ee24">computeLoss</a> (const ActivationFunction &amp;act_func, const MyMatrix &amp;X, const MyVector &amp;Y, const std::vector&lt; MyMatrix &gt; &amp;W, const std::vector&lt; MyVector &gt; &amp;B, const <a class="el" href="struct_params.html">Params</a> params, double &amp;loss, double &amp;accuracy)</td></tr>
<tr class="memdesc:ad4e3600f69eecf523fb78bb9ffd4ee24"><td class="mdescLeft">&#160;</td><td class="mdescRight">function that compute the Negative Log-likelihood and the accuracy of the model (cannot be used with dropout since the mask will be activated!)  <a href="#ad4e3600f69eecf523fb78bb9ffd4ee24">More...</a><br /></td></tr>
<tr class="separator:ad4e3600f69eecf523fb78bb9ffd4ee24"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a99cafbd2955aafe43ad57a88f7f1db5c"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a99cafbd2955aafe43ad57a88f7f1db5c">evalModel</a> (const ActivationFunction &amp;eval_act_func, const MyMatrix &amp;X_train, const MyMatrix &amp;Y_train, const MyMatrix &amp;X_valid, const MyMatrix &amp;Y_valid, const <a class="el" href="struct_params.html">Params</a> &amp;params, std::vector&lt; MyMatrix &gt; W_eval, std::vector&lt; MyVector &gt; B, double &amp;train_loss, double &amp;train_accuracy, double &amp;valid_loss, double &amp;valid_accuracy)</td></tr>
<tr class="memdesc:a99cafbd2955aafe43ad57a88f7f1db5c"><td class="mdescLeft">&#160;</td><td class="mdescRight">function that evaluate the model on the training dataset and the validation dataset  <a href="#a99cafbd2955aafe43ad57a88f7f1db5c">More...</a><br /></td></tr>
<tr class="separator:a99cafbd2955aafe43ad57a88f7f1db5c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac98a75118cb5c283016ee1d3009e4591"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#ac98a75118cb5c283016ee1d3009e4591">updateStepsize</a> (const unsigned t, const double init_eta, double &amp;eta)</td></tr>
<tr class="memdesc:ac98a75118cb5c283016ee1d3009e4591"><td class="mdescLeft">&#160;</td><td class="mdescRight">simple implementation of a decreasing step-size for ensuring convergence  <a href="#ac98a75118cb5c283016ee1d3009e4591">More...</a><br /></td></tr>
<tr class="separator:ac98a75118cb5c283016ee1d3009e4591"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa0628205a0dd1fb71743bcea4ff051d5"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#aa0628205a0dd1fb71743bcea4ff051d5">buildDiagMetric</a> (const std::vector&lt; MyMatrix &gt; &amp;gradB_sq, const std::vector&lt; MyMatrix &gt; &amp;A, const MyMatrix &amp;X_batch, const std::vector&lt; MyMatrix &gt; &amp;W, const double mat_reg, std::vector&lt; MyMatrix &gt; &amp;Mii, std::vector&lt; MyVector &gt; &amp;M00)</td></tr>
<tr class="memdesc:aa0628205a0dd1fb71743bcea4ff051d5"><td class="mdescLeft">&#160;</td><td class="mdescRight">Function that builds a diagonal metric from the gradient w.r.t. the pre-activation.  <a href="#aa0628205a0dd1fb71743bcea4ff051d5">More...</a><br /></td></tr>
<tr class="separator:aa0628205a0dd1fb71743bcea4ff051d5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a873ca1a30adcb11f4d90563b14139122"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a873ca1a30adcb11f4d90563b14139122">buildQDMetric</a> (const std::vector&lt; MyMatrix &gt; &amp;gradB_sq, const std::vector&lt; MyMatrix &gt; &amp;A, const MyMatrix &amp;X_batch, const std::vector&lt; MyMatrix &gt; &amp;W, const double mat_reg, std::vector&lt; MyMatrix &gt; &amp;Mii, std::vector&lt; MyMatrix &gt; &amp;M0i, std::vector&lt; MyVector &gt; &amp;M00)</td></tr>
<tr class="memdesc:a873ca1a30adcb11f4d90563b14139122"><td class="mdescLeft">&#160;</td><td class="mdescRight">Function that builds a quasi-diagonal metric from the gradient w.r.t. the pre-activation.  <a href="#a873ca1a30adcb11f4d90563b14139122">More...</a><br /></td></tr>
<tr class="separator:a873ca1a30adcb11f4d90563b14139122"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8b089ff4f32d95bcacc9a7cfa7bc1ef6"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a8b089ff4f32d95bcacc9a7cfa7bc1ef6">updateMetric</a> (const bool init_flag, const double gamma, std::vector&lt; MyMatrix &gt; &amp;Mii, std::vector&lt; MyMatrix &gt; &amp;M0i, std::vector&lt; MyVector &gt; &amp;M00, std::vector&lt; MyMatrix &gt; &amp;pMii, std::vector&lt; MyMatrix &gt; &amp;pM0i, std::vector&lt; MyVector &gt; &amp;pM00)</td></tr>
<tr class="memdesc:a8b089ff4f32d95bcacc9a7cfa7bc1ef6"><td class="mdescLeft">&#160;</td><td class="mdescRight">Update the quasi-diagonal approximation of the metric with an exponential moving average.  <a href="#a8b089ff4f32d95bcacc9a7cfa7bc1ef6">More...</a><br /></td></tr>
<tr class="separator:a8b089ff4f32d95bcacc9a7cfa7bc1ef6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:afc59d7c69fc14a16d2fb47f309f3f63b"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#afc59d7c69fc14a16d2fb47f309f3f63b">updateMetric</a> (const bool init_flag, const double gamma, std::vector&lt; MyMatrix &gt; &amp;Mii, std::vector&lt; MyVector &gt; &amp;M00, std::vector&lt; MyMatrix &gt; &amp;pMii, std::vector&lt; MyVector &gt; &amp;pM00)</td></tr>
<tr class="memdesc:afc59d7c69fc14a16d2fb47f309f3f63b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Update the diagonal approximation of the metric with an exponential moving average.  <a href="#afc59d7c69fc14a16d2fb47f309f3f63b">More...</a><br /></td></tr>
<tr class="separator:afc59d7c69fc14a16d2fb47f309f3f63b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5aaf33307c7448b0ed1991deafc24db1"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a5aaf33307c7448b0ed1991deafc24db1">qdGradient</a> (const MyMatrix &amp;Mii, const MyMatrix &amp;M0i, const MyVector &amp;M00, const MyMatrix &amp;dW, const MyVector &amp;dB, MyMatrix &amp;qd_dW, MyVector &amp;qd_dB)</td></tr>
<tr class="memdesc:a5aaf33307c7448b0ed1991deafc24db1"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute the natural gradient with the quasi-diagonal approximation.  <a href="#a5aaf33307c7448b0ed1991deafc24db1">More...</a><br /></td></tr>
<tr class="separator:a5aaf33307c7448b0ed1991deafc24db1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af144ec609311b53a6c4d0cb6678932b9"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#af144ec609311b53a6c4d0cb6678932b9">diagGradient</a> (const MyMatrix &amp;Mii, const MyVector &amp;M00, const MyMatrix &amp;dW, const MyVector &amp;dB, MyMatrix &amp;qd_dW, MyVector &amp;qd_dB)</td></tr>
<tr class="memdesc:af144ec609311b53a6c4d0cb6678932b9"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute the natural gradient with the diagonal approximation.  <a href="#af144ec609311b53a6c4d0cb6678932b9">More...</a><br /></td></tr>
<tr class="separator:af144ec609311b53a6c4d0cb6678932b9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a5a372ad6a43f1c26470b5125125986ad"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a5a372ad6a43f1c26470b5125125986ad">update</a> (const double eta, const std::vector&lt; MyMatrix &gt; &amp;gradB, const std::vector&lt; MyMatrix &gt; &amp;A, const MyMatrix &amp;X_batch, const std::string regularizer, const double lambda, std::vector&lt; MyMatrix &gt; &amp;W, std::vector&lt; MyVector &gt; &amp;B, std::vector&lt; MyMatrix &gt; &amp;Mii, std::vector&lt; MyMatrix &gt; &amp;M0i, std::vector&lt; MyVector &gt; &amp;M00)</td></tr>
<tr class="memdesc:a5a372ad6a43f1c26470b5125125986ad"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the update function of the quasi-diagonal natural gradient descent algorithm.  <a href="#a5a372ad6a43f1c26470b5125125986ad">More...</a><br /></td></tr>
<tr class="separator:a5a372ad6a43f1c26470b5125125986ad"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a2be07c086229dee20135008288130b0c"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#a2be07c086229dee20135008288130b0c">update</a> (const double eta, const std::vector&lt; MyMatrix &gt; &amp;gradB, const std::vector&lt; MyMatrix &gt; &amp;A, const MyMatrix &amp;X_batch, const std::string regularizer, const double lambda, std::vector&lt; MyMatrix &gt; &amp;W, std::vector&lt; MyVector &gt; &amp;B, std::vector&lt; MyMatrix &gt; &amp;Mii, std::vector&lt; MyVector &gt; &amp;M00)</td></tr>
<tr class="memdesc:a2be07c086229dee20135008288130b0c"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the update function of the diagonal natural gradient descent algorithm.  <a href="#a2be07c086229dee20135008288130b0c">More...</a><br /></td></tr>
<tr class="separator:a2be07c086229dee20135008288130b0c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:abdf2ed53acfc5b2df65b85c1f5d1970f"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#abdf2ed53acfc5b2df65b85c1f5d1970f">adaptiveRule</a> (const double train_loss, double &amp;prev_loss, double &amp;eta, std::vector&lt; MyMatrix &gt; &amp;W, std::vector&lt; MyVector &gt; &amp;B, std::vector&lt; MyMatrix &gt; &amp;pMii, std::vector&lt; MyMatrix &gt; &amp;pM0i, std::vector&lt; MyVector &gt; &amp;pM00, std::vector&lt; MyMatrix &gt; &amp;pW, std::vector&lt; MyVector &gt; &amp;pB, std::vector&lt; MyMatrix &gt; &amp;ppMii, std::vector&lt; MyMatrix &gt; &amp;ppM0i, std::vector&lt; MyVector &gt; &amp;ppM00)</td></tr>
<tr class="memdesc:abdf2ed53acfc5b2df65b85c1f5d1970f"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of a simple 0.5/1.2 step-size adaptation rule.  <a href="#abdf2ed53acfc5b2df65b85c1f5d1970f">More...</a><br /></td></tr>
<tr class="separator:abdf2ed53acfc5b2df65b85c1f5d1970f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aa315c58532f165c406aa803c1524a9a8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#aa315c58532f165c406aa803c1524a9a8">qdBpmBprop</a> (const std::vector&lt; MyMatrix &gt; &amp;W, const std::vector&lt; MyMatrix &gt; &amp;dA, std::vector&lt; MyMatrix &gt; &amp;bp_gradB)</td></tr>
<tr class="memdesc:aa315c58532f165c406aa803c1524a9a8"><td class="mdescLeft">&#160;</td><td class="mdescRight">Implementation of the metric backpropagation algorithm.  <a href="#aa315c58532f165c406aa803c1524a9a8">More...</a><br /></td></tr>
<tr class="separator:aa315c58532f165c406aa803c1524a9a8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ab5839ec0cc5d7bcbf94b68562f365bee"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="nn__ops_8hpp.html#ab5839ec0cc5d7bcbf94b68562f365bee">computeMcError</a> (const MyMatrix &amp;out, MyMatrix &amp;mc_error)</td></tr>
<tr class="memdesc:ab5839ec0cc5d7bcbf94b68562f365bee"><td class="mdescLeft">&#160;</td><td class="mdescRight">Function that sample a label randomly according to the output distribution and evaluate the gradient w.r.t. to the output This function is required by the algorithm qdMCNat.  <a href="#ab5839ec0cc5d7bcbf94b68562f365bee">More...</a><br /></td></tr>
<tr class="separator:ab5839ec0cc5d7bcbf94b68562f365bee"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a098dfb7eceed6e3e86c4866ffd2369d1"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a098dfb7eceed6e3e86c4866ffd2369d1"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><b>initSlidingMetric</b> (const std::vector&lt; MyMatrix &gt; &amp;W, std::vector&lt; MyMatrix &gt; &amp;pMii, std::vector&lt; MyMatrix &gt; &amp;pM0i, std::vector&lt; MyVector &gt; &amp;pM00)</td></tr>
<tr class="separator:a098dfb7eceed6e3e86c4866ffd2369d1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a63eea3fc804396e7f95d93380c997358"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a63eea3fc804396e7f95d93380c997358"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><b>computeISError</b> (const MyMatrix &amp;out, const MyMatrix &amp;one_hot_batch, const double q, MyMatrix &amp;mc_error)</td></tr>
<tr class="separator:a63eea3fc804396e7f95d93380c997358"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a41422fe592bc3974f9dbcde6e1422bfa"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a41422fe592bc3974f9dbcde6e1422bfa"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><b>computeLazyError</b> (const MyMatrix &amp;out, const MyMatrix &amp;one_hot_batch, MyMatrix &amp;lazy_error)</td></tr>
<tr class="separator:a41422fe592bc3974f9dbcde6e1422bfa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4bc298a63450bce97f16047a8be8c0cb"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a4bc298a63450bce97f16047a8be8c0cb"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><b>testUpdate</b> (const double eta, const std::vector&lt; MyMatrix &gt; &amp;gradB, const std::vector&lt; MyMatrix &gt; &amp;A, const MyMatrix &amp;X_batch, const std::string regularizer, const double lambda, std::vector&lt; MyMatrix &gt; &amp;W, std::vector&lt; MyVector &gt; &amp;B, std::vector&lt; MyMatrix &gt; &amp;DW, std::vector&lt; MyVector &gt; &amp;DB)</td></tr>
<tr class="separator:a4bc298a63450bce97f16047a8be8c0cb"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a07d054f437b06f4d5a4804fd29874b46"><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a07d054f437b06f4d5a4804fd29874b46"></a>
void&#160;</td><td class="memItemRight" valign="bottom"><b>updateTest</b> (const double eta, const std::vector&lt; MyMatrix &gt; &amp;gradB, const std::vector&lt; MyMatrix &gt; &amp;A, const MyMatrix &amp;X_batch, const std::string regularizer, const double lambda, std::vector&lt; MyMatrix &gt; &amp;W, std::vector&lt; MyVector &gt; &amp;B, std::vector&lt; MyMatrix &gt; &amp;Mii, std::vector&lt; MyMatrix &gt; &amp;M0i, std::vector&lt; MyVector &gt; &amp;M00)</td></tr>
<tr class="separator:a07d054f437b06f4d5a4804fd29874b46"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Implementation of the functions required for neural networks. </p>
<dl class="section author"><dt>Author</dt><dd>Gaetan Marceau Caron &amp; Yann Ollivier </dd></dl>
<dl class="section version"><dt>Version</dt><dd>1.0 </dd></dl>
</div><h2 class="groupheader">Function Documentation</h2>
<a class="anchor" id="a3c2822a2bbb5ca3701af33bbc402e54f"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void adagradUpdate </td>
          <td>(</td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>eta</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>gradB</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>A</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>X_batch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string&#160;</td>
          <td class="paramname"><em>regularizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>lambda</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>mat_reg</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>autocorr</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>B</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>mu_dW</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>mu_dB</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>function that performs the adagrad update rule </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">eta</td><td>the gradient descent step-size </td></tr>
    <tr><td class="paramname">gradB</td><td>a standard vector of the gradient w.r.t. the pre-activation values (one per layer) </td></tr>
    <tr><td class="paramname">A</td><td>a standard vector of activation values (one per layer) </td></tr>
    <tr><td class="paramname">X_batch</td><td>a matrix containing the training examples of the current minibatch </td></tr>
    <tr><td class="paramname">regularizer</td><td>the string of the norm regularizer (L1 or L2) </td></tr>
    <tr><td class="paramname">lambda</td><td>the amplitude of the regularization term </td></tr>
    <tr><td class="paramname">mat_reg</td><td>a numerical regularization term s.t. there is no division by zero </td></tr>
    <tr><td class="paramname">autocorr</td><td>the autocorrelation coefficient of the adagrad update rule </td></tr>
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">B</td><td>a standard vector of bias vectors (one per layer) </td></tr>
    <tr><td class="paramname">mu_dW</td><td>a standard vector of the rolling averages of the weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">mu_dB</td><td>a standard vector of the rolling averages of the bias vectors (one per layer) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="abdf2ed53acfc5b2df65b85c1f5d1970f"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void adaptiveRule </td>
          <td>(</td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>train_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double &amp;&#160;</td>
          <td class="paramname"><em>prev_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double &amp;&#160;</td>
          <td class="paramname"><em>eta</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>B</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>pMii</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>pM0i</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>pM00</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>pW</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>pB</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>ppMii</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>ppM0i</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>ppM00</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of a simple 0.5/1.2 step-size adaptation rule. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">train_loss</td><td>the negative log-likelihood associated to the current parameters </td></tr>
    <tr><td class="paramname">prev_loss</td><td>the negative log-likelihood associated to the previous parameters </td></tr>
    <tr><td class="paramname">eta</td><td>the current step-size </td></tr>
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">B</td><td>a standard vector of bias vectors (one per layer) </td></tr>
    <tr><td class="paramname">pMii</td><td>a standard vector of the exponential moving average of Mii </td></tr>
    <tr><td class="paramname">pMi0</td><td>a standard vector of the exponential moving average of M0i </td></tr>
    <tr><td class="paramname">pM00</td><td>a standard vector of the exponential moving average of M00 </td></tr>
    <tr><td class="paramname">pW</td><td>a standard vector of weight matrices of the previous iteration (one per layer) </td></tr>
    <tr><td class="paramname">pB</td><td>a standard vector of bias vectors of the previous iteration (one per layer) </td></tr>
    <tr><td class="paramname">ppMii</td><td>a standard vector of the exponential moving average of Mii of the previous iteration </td></tr>
    <tr><td class="paramname">ppMi0</td><td>a standard vector of the exponential moving average of M0i of the previous iteration </td></tr>
    <tr><td class="paramname">ppM00</td><td>a standard vector of the exponential moving average of M00 of the previous iteration </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a1c7cf6b20b44ba081abe2a5e0110a599"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void bprop </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>dA</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>gradB</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the backpropagation algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">dA</td><td>a standard vector of the derivatives of the activation values (one per layer) </td></tr>
    <tr><td class="paramname">gradB</td><td>a standard vector of the gradient w.r.t. the pre-activation values (one per layer) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="aa0628205a0dd1fb71743bcea4ff051d5"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void buildDiagMetric </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>gradB_sq</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>A</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>X_batch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>mat_reg</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>Mii</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>M00</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Function that builds a diagonal metric from the gradient w.r.t. the pre-activation. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">gradB_sq</td><td>a standard vector of the square of the gradient w.r.t. the pre-activation values (one per layer) </td></tr>
    <tr><td class="paramname">A</td><td>a standard vector of activation values (one per layer) </td></tr>
    <tr><td class="paramname">X_batch</td><td>a matrix containing the training examples of the current minibatch </td></tr>
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">mat_reg</td><td>a numerical regularization term s.t. the metric is invertible </td></tr>
    <tr><td class="paramname">Mii</td><td>a standard vector of the diagonals of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">M00</td><td>a standard vector of the bias of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a873ca1a30adcb11f4d90563b14139122"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void buildQDMetric </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>gradB_sq</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>A</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>X_batch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>mat_reg</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>Mii</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>M0i</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>M00</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Function that builds a quasi-diagonal metric from the gradient w.r.t. the pre-activation. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">gradB_sq</td><td>a standard vector of the square of the gradient w.r.t. the pre-activation values (one per layer) </td></tr>
    <tr><td class="paramname">A</td><td>a standard vector of activation values (one per layer) </td></tr>
    <tr><td class="paramname">X_batch</td><td>a matrix containing the training examples of the current minibatch </td></tr>
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">mat_reg</td><td>a numerical regularization term s.t. the metric is invertible </td></tr>
    <tr><td class="paramname">Mii</td><td>a standard vector of the diagonals of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">Mi0</td><td>a standard vector of the weights time the bias (first line) of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">M00</td><td>a standard vector of the bias of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a9a994e746ada8c2bd3f11b68114db234"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void computeLoss </td>
          <td>(</td>
          <td class="paramtype">const ActivationFunction &amp;&#160;</td>
          <td class="paramname"><em>act_func</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyVector &amp;&#160;</td>
          <td class="paramname"><em>Y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>B</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double &amp;&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double &amp;&#160;</td>
          <td class="paramname"><em>accuracy</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>function that compute the Negative Log-likelihood and the accuracy of the model </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">act_func</td><td>the reference of the activation function </td></tr>
    <tr><td class="paramname">X</td><td>a matrix containing the training examples </td></tr>
    <tr><td class="paramname">Y</td><td>a vector containing the labels of the examples </td></tr>
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">B</td><td>a standard vector of bias vectors (one per layer) </td></tr>
    <tr><td class="paramname">loss</td><td>the negative log-likelihood </td></tr>
    <tr><td class="paramname">accuracy</td><td>the accuracy </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ad4e3600f69eecf523fb78bb9ffd4ee24"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void computeLoss </td>
          <td>(</td>
          <td class="paramtype">const ActivationFunction &amp;&#160;</td>
          <td class="paramname"><em>act_func</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>X</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyVector &amp;&#160;</td>
          <td class="paramname"><em>Y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>B</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="struct_params.html">Params</a>&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double &amp;&#160;</td>
          <td class="paramname"><em>loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double &amp;&#160;</td>
          <td class="paramname"><em>accuracy</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>function that compute the Negative Log-likelihood and the accuracy of the model (cannot be used with dropout since the mask will be activated!) </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">act_func</td><td>the reference of the activation function </td></tr>
    <tr><td class="paramname">X</td><td>a matrix containing the training examples </td></tr>
    <tr><td class="paramname">Y</td><td>a vector containing the labels of the examples </td></tr>
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">B</td><td>a standard vector of bias vectors (one per layer) </td></tr>
    <tr><td class="paramname">params</td><td>the parameters of the program </td></tr>
    <tr><td class="paramname">loss</td><td>the negative log-likelihood </td></tr>
    <tr><td class="paramname">accuracy</td><td>the accuracy </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ab5839ec0cc5d7bcbf94b68562f365bee"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void computeMcError </td>
          <td>(</td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>out</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>mc_error</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Function that sample a label randomly according to the output distribution and evaluate the gradient w.r.t. to the output This function is required by the algorithm qdMCNat. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">out</td><td>the probability distribution over the output (values returned by softmax) </td></tr>
    <tr><td class="paramname">mc_error</td><td>the gradient w.r.t. to the output </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="af144ec609311b53a6c4d0cb6678932b9"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void diagGradient </td>
          <td>(</td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>Mii</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyVector &amp;&#160;</td>
          <td class="paramname"><em>M00</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>dW</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyVector &amp;&#160;</td>
          <td class="paramname"><em>dB</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>qd_dW</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyVector &amp;&#160;</td>
          <td class="paramname"><em>qd_dB</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute the natural gradient with the diagonal approximation. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">Mii</td><td>the diagonals of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">M00</td><td>the bias of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">dW</td><td>the gradient w.r.t. the weights </td></tr>
    <tr><td class="paramname">dB</td><td>the gradient w.r.t. the bias </td></tr>
    <tr><td class="paramname">qd_dW</td><td>the quasi-diagonal natural gradient w.r.t. the weights </td></tr>
    <tr><td class="paramname">qd_dB</td><td>the quasi-diagonal natural gradient w.r.t. the bias </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ac56dfda8be864dadf5db85c56978171d"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void dropout </td>
          <td>(</td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>prob</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>A</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>B</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the dropout regularization technique. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">prob</td><td>the probability of dropout </td></tr>
    <tr><td class="paramname">A</td><td>a matrix to regularize (n_example X n_unit) </td></tr>
    <tr><td class="paramname">B</td><td>another matrix to regularize with the same mask (n_example X n_unit) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="afe6393d9948ae9a81cb1292aad353c48"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void dropout </td>
          <td>(</td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>prob</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>A</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the dropout regularization technique. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">prob</td><td>the probability of dropout </td></tr>
    <tr><td class="paramname">A</td><td>a matrix to regularize (n_example X n_unit) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a99cafbd2955aafe43ad57a88f7f1db5c"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void evalModel </td>
          <td>(</td>
          <td class="paramtype">const ActivationFunction &amp;&#160;</td>
          <td class="paramname"><em>eval_act_func</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>X_train</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>Y_train</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>X_valid</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>Y_valid</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const <a class="el" href="struct_params.html">Params</a> &amp;&#160;</td>
          <td class="paramname"><em>params</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt;&#160;</td>
          <td class="paramname"><em>W_eval</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt;&#160;</td>
          <td class="paramname"><em>B</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double &amp;&#160;</td>
          <td class="paramname"><em>train_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double &amp;&#160;</td>
          <td class="paramname"><em>train_accuracy</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double &amp;&#160;</td>
          <td class="paramname"><em>valid_loss</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double &amp;&#160;</td>
          <td class="paramname"><em>valid_accuracy</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>function that evaluate the model on the training dataset and the validation dataset </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">eval_act_func</td><td>the reference of the activation function </td></tr>
    <tr><td class="paramname">X_train</td><td>a matrix containing the training examples </td></tr>
    <tr><td class="paramname">Y_train</td><td>a vector containing the labels of the training examples </td></tr>
    <tr><td class="paramname">X_valid</td><td>a matrix containing the validation examples </td></tr>
    <tr><td class="paramname">Y_valid</td><td>a vector containing the labels of the validation examples </td></tr>
    <tr><td class="paramname">params</td><td>the parameters of the program </td></tr>
    <tr><td class="paramname">W_eval</td><td>a copy of the standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">B</td><td>a standard vector of bias vectors (one per layer) </td></tr>
    <tr><td class="paramname">train_loss</td><td>the negative log-likelihood on the training set </td></tr>
    <tr><td class="paramname">train_accuracy</td><td>the accuracy on the training set </td></tr>
    <tr><td class="paramname">valid_loss</td><td>the negative log-likelihood on the validation set </td></tr>
    <tr><td class="paramname">valid_accuracy</td><td>the accuracy on the validation set </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a333e03b7cdb8f33e9619edf27f56d6cb"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void fprop </td>
          <td>(</td>
          <td class="paramtype">const bool&#160;</td>
          <td class="paramname"><em>dropout_flag</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>dropout_prob</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const ActivationFunction &amp;&#160;</td>
          <td class="paramname"><em>act_func</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>B</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>X_batch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>Z</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>A</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>dA</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the forward propagation algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">dropout_flag</td><td>the flag for activating dropout regularization </td></tr>
    <tr><td class="paramname">dropout_prob</td><td>the probability of dropout (dropout_flag must be true) </td></tr>
    <tr><td class="paramname">act_func</td><td>the reference of the activation function </td></tr>
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">B</td><td>a standard vector of bias vectors (one per layer) </td></tr>
    <tr><td class="paramname">X_batch</td><td>a matrix containing the training examples of the current minibatch </td></tr>
    <tr><td class="paramname">Z</td><td>a standard vector of pre-activation values (one per layer) </td></tr>
    <tr><td class="paramname">A</td><td>a standard vector of activation values (one per layer) </td></tr>
    <tr><td class="paramname">dA</td><td>a standard vector of the derivatives of the activation values (one per layer) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a2a07778f5ff97c5d5581276d663ede6e"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int initNetwork </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; unsigned &gt; &amp;&#160;</td>
          <td class="paramname"><em>nn_arch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string&#160;</td>
          <td class="paramname"><em>act_func</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>B</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Initialize the parameters of the Neural Network computational graph. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">nn_arch</td><td>number of activation units for each layer </td></tr>
    <tr><td class="paramname">act_func</td><td>string representing the activation function </td></tr>
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">B</td><td>a standard vector of bias vectors (one per layer) </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the number of parameters </dd></dl>

</div>
</div>
<a class="anchor" id="a90b8525c0edded9e3420dc56dbb9332f"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void initWeight </td>
          <td>(</td>
          <td class="paramtype">const unsigned&#160;</td>
          <td class="paramname"><em>n_act0</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const unsigned&#160;</td>
          <td class="paramname"><em>n_act1</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>sigma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>W</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Initialize the weights of a layer with a reweighted normal noise. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">n_act0</td><td>number of input units </td></tr>
    <tr><td class="paramname">n_act1</td><td>number of output units </td></tr>
    <tr><td class="paramname">sigma</td><td>stddev of the normal noise </td></tr>
    <tr><td class="paramname">W</td><td>weight matrix returned by the function </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a1ee99de11ae8d8f50b32c2037ffd1d1b"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void logistic </td>
          <td>(</td>
          <td class="paramtype">const bool&#160;</td>
          <td class="paramname"><em>deriv_flag</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>z</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>da</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the logistic function and its derivative. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">deriv_flag</td><td>a flag for computing the derivative at the same time </td></tr>
    <tr><td class="paramname">z</td><td>the pre-activation matrix (n_example X n_unit) </td></tr>
    <tr><td class="paramname">a</td><td>the activation matrix (n_example X n_unit) </td></tr>
    <tr><td class="paramname">da</td><td>the derivative activation matrix (n_example X n_unit) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="aa8e7c94ef2b705a126e52d0ea2fa9901"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void my_tanh </td>
          <td>(</td>
          <td class="paramtype">const bool&#160;</td>
          <td class="paramname"><em>deriv_flag</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>z</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>da</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the tanh function and its derivative. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">deriv_flag</td><td>a flag for computing the derivative at the same time </td></tr>
    <tr><td class="paramname">z</td><td>the pre-activation matrix (n_example X n_unit) </td></tr>
    <tr><td class="paramname">a</td><td>the activation matrix (n_example X n_unit) </td></tr>
    <tr><td class="paramname">da</td><td>the derivative activation matrix (n_example X n_unit) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="aa315c58532f165c406aa803c1524a9a8"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void qdBpmBprop </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>dA</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>bp_gradB</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the metric backpropagation algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">dA</td><td>a standard vector of the derivatives of the activation values (one per layer) </td></tr>
    <tr><td class="paramname">bp_gradB</td><td>a standard vector of the backpropagated metric (one per layer) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a5aaf33307c7448b0ed1991deafc24db1"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void qdGradient </td>
          <td>(</td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>Mii</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>M0i</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyVector &amp;&#160;</td>
          <td class="paramname"><em>M00</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>dW</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyVector &amp;&#160;</td>
          <td class="paramname"><em>dB</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>qd_dW</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyVector &amp;&#160;</td>
          <td class="paramname"><em>qd_dB</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute the natural gradient with the quasi-diagonal approximation. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">Mii</td><td>the diagonals of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">Mi0</td><td>the weights time the bias (first line) of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">M00</td><td>the bias of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">dW</td><td>the gradient w.r.t. the weights </td></tr>
    <tr><td class="paramname">dB</td><td>the gradient w.r.t. the bias </td></tr>
    <tr><td class="paramname">qd_dW</td><td>the quasi-diagonal natural gradient w.r.t. the weights </td></tr>
    <tr><td class="paramname">qd_dB</td><td>the quasi-diagonal natural gradient w.r.t. the bias </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="af47e86a1783f4c04ee25301f3ec37866"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void relu </td>
          <td>(</td>
          <td class="paramtype">const bool&#160;</td>
          <td class="paramname"><em>deriv_flag</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>z</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>da</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the rectified linear unit (ReLU) function and its derivative. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">deriv_flag</td><td>a flag for computing the derivative at the same time </td></tr>
    <tr><td class="paramname">z</td><td>the pre-activation matrix (n_example X n_unit) </td></tr>
    <tr><td class="paramname">a</td><td>the activation matrix (n_example X n_unit) </td></tr>
    <tr><td class="paramname">da</td><td>the derivative activation matrix (n_example X n_unit) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a22f109ce3d23796de9d34fb9eee73040"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double signFunc </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the sign function. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">x</td><td>an input value </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the sign of x </dd></dl>

</div>
</div>
<a class="anchor" id="a4c86f2557fe76d7b0ca7c5111eb7927b"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void softmax </td>
          <td>(</td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>out</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the softmax function Take a matrix (n_example X n_class) of values and return a matrix (n_example X n_class) where each line is a probability distribution over the classes. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">a</td><td>the pre-activation values </td></tr>
    <tr><td class="paramname">out</td><td>the probability distribution </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a6ceeb10733d64ab2587e136e884c2ec8"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double sqrtFunc </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the square-root function. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">x</td><td>the pre-activation value </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the square-root of x </dd></dl>

</div>
</div>
<a class="anchor" id="a03a2f1c37574a2bc61630b6e5bf86dcf"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double squareFunc </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>x</em></td><td>)</td>
          <td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the square function. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">x</td><td>the pre-activation value </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the square of x </dd></dl>

</div>
</div>
<a class="anchor" id="a6bac3899a024aa93b77e99e169f1e86c"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double sumCstFunc </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>cst</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the translation function. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">x</td><td>the pre-activation value </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>x plus a constant </dd></dl>

</div>
</div>
<a class="anchor" id="ac8a4ddb1d085a6e8f288882dc5e2273f"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void update </td>
          <td>(</td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>eta</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>gradB</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>A</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>X_batch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string&#160;</td>
          <td class="paramname"><em>regularizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>lambda</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>B</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the update function of the gradient descent algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">eta</td><td>the gradient descent step-size </td></tr>
    <tr><td class="paramname">gradB</td><td>a standard vector of the gradient w.r.t. the pre-activation values (one per layer) </td></tr>
    <tr><td class="paramname">A</td><td>a standard vector of activation values (one per layer) </td></tr>
    <tr><td class="paramname">X_batch</td><td>a matrix containing the training examples of the current minibatch </td></tr>
    <tr><td class="paramname">regularizer</td><td>the string of the norm regularizer (L1 or L2) </td></tr>
    <tr><td class="paramname">lambda</td><td>the amplitude of the regularization term </td></tr>
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">B</td><td>a standard vector of bias vectors (one per layer) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a5a372ad6a43f1c26470b5125125986ad"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void update </td>
          <td>(</td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>eta</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>gradB</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>A</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>X_batch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string&#160;</td>
          <td class="paramname"><em>regularizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>lambda</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>B</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>Mii</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>M0i</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>M00</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the update function of the quasi-diagonal natural gradient descent algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">eta</td><td>the gradient descent step-size </td></tr>
    <tr><td class="paramname">gradB</td><td>a standard vector of the gradient w.r.t. the pre-activation values (one per layer) </td></tr>
    <tr><td class="paramname">A</td><td>a standard vector of activation values (one per layer) </td></tr>
    <tr><td class="paramname">X_batch</td><td>a matrix containing the training examples of the current minibatch </td></tr>
    <tr><td class="paramname">regularizer</td><td>the string of the norm regularizer (L1 or L2) </td></tr>
    <tr><td class="paramname">lambda</td><td>the amplitude of the regularization term </td></tr>
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">B</td><td>a standard vector of bias vectors (one per layer) </td></tr>
    <tr><td class="paramname">Mii</td><td>the diagonals of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">Mi0</td><td>the weights time the bias (first line) of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">M00</td><td>the bias of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a2be07c086229dee20135008288130b0c"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void update </td>
          <td>(</td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>eta</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>gradB</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>A</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const MyMatrix &amp;&#160;</td>
          <td class="paramname"><em>X_batch</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string&#160;</td>
          <td class="paramname"><em>regularizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>lambda</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>W</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>B</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>Mii</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>M00</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Implementation of the update function of the diagonal natural gradient descent algorithm. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">eta</td><td>the gradient descent step-size </td></tr>
    <tr><td class="paramname">gradB</td><td>a standard vector of the gradient w.r.t. the pre-activation values (one per layer) </td></tr>
    <tr><td class="paramname">A</td><td>a standard vector of activation values (one per layer) </td></tr>
    <tr><td class="paramname">X_batch</td><td>a matrix containing the training examples of the current minibatch </td></tr>
    <tr><td class="paramname">regularizer</td><td>the string of the norm regularizer (L1 or L2) </td></tr>
    <tr><td class="paramname">lambda</td><td>the amplitude of the regularization term </td></tr>
    <tr><td class="paramname">W</td><td>a standard vector of weight matrices (one per layer) </td></tr>
    <tr><td class="paramname">B</td><td>a standard vector of bias vectors (one per layer) </td></tr>
    <tr><td class="paramname">Mii</td><td>the diagonals of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">M00</td><td>the bias of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a8b089ff4f32d95bcacc9a7cfa7bc1ef6"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void updateMetric </td>
          <td>(</td>
          <td class="paramtype">const bool&#160;</td>
          <td class="paramname"><em>init_flag</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>Mii</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>M0i</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>M00</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>pMii</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>pM0i</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>pM00</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Update the quasi-diagonal approximation of the metric with an exponential moving average. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">init_flag</td><td>a flag for initializing the exponential moving average </td></tr>
    <tr><td class="paramname">gamma</td><td>the coefficient of the exponential moving average </td></tr>
    <tr><td class="paramname">Mii</td><td>a standard vector of the diagonals of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">Mi0</td><td>a standard vector of the weights time the bias (first line) of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">M00</td><td>a standard vector of the bias of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">pMii</td><td>a standard vector of the previous exponential moving average of Mii </td></tr>
    <tr><td class="paramname">pMi0</td><td>a standard vector of the previous exponential moving average of M0i </td></tr>
    <tr><td class="paramname">pM00</td><td>a standard vector of the previous exponential moving average of M00 </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="afc59d7c69fc14a16d2fb47f309f3f63b"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void updateMetric </td>
          <td>(</td>
          <td class="paramtype">const bool&#160;</td>
          <td class="paramname"><em>init_flag</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>gamma</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>Mii</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>M00</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyMatrix &gt; &amp;&#160;</td>
          <td class="paramname"><em>pMii</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; MyVector &gt; &amp;&#160;</td>
          <td class="paramname"><em>pM00</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Update the diagonal approximation of the metric with an exponential moving average. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">init_flag</td><td>a flag for initializing the exponential moving average </td></tr>
    <tr><td class="paramname">gamma</td><td>the coefficient of the exponential moving average </td></tr>
    <tr><td class="paramname">Mii</td><td>a standard vector of the diagonals of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">M00</td><td>a standard vector of the bias of the block sub-matrices of the Riemannian metric (one per layer) </td></tr>
    <tr><td class="paramname">pMii</td><td>a standard vector of the previous exponential moving average of Mii </td></tr>
    <tr><td class="paramname">pM00</td><td>a standard vector of the previous exponential moving average of M00 </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="aaa5dfcccaa4f859b162005465984f40e"></a>
<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;class T &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">void updateParam </td>
          <td>(</td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>eta</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::string&#160;</td>
          <td class="paramname"><em>regularizer</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>lambda</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const T &amp;&#160;</td>
          <td class="paramname"><em>dparams</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">T &amp;&#160;</td>
          <td class="paramname"><em>params</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Templated function for updating matrices or vectors. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">eta</td><td>the gradient descent step-size </td></tr>
    <tr><td class="paramname">regularizer</td><td>the string of the norm regularizer (L1 or L2) </td></tr>
    <tr><td class="paramname">lambda</td><td>the amplitude of the regularization term </td></tr>
    <tr><td class="paramname">dparams</td><td>a matrix/vector containing the updates </td></tr>
    <tr><td class="paramname">params</td><td>a matrix/vector to be updated </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ac98a75118cb5c283016ee1d3009e4591"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void updateStepsize </td>
          <td>(</td>
          <td class="paramtype">const unsigned&#160;</td>
          <td class="paramname"><em>t</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const double&#160;</td>
          <td class="paramname"><em>init_eta</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double &amp;&#160;</td>
          <td class="paramname"><em>eta</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>simple implementation of a decreasing step-size for ensuring convergence </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">t</td><td>number of iterations </td></tr>
    <tr><td class="paramname">init_eta</td><td>initial step-size </td></tr>
    <tr><td class="paramname">eta</td><td>adjusted step-size </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Tue Oct 27 2015 14:23:59 for DNN by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.9.1
</small></address>
</body>
</html>
